
# 线性回归

## 一、问题

### 1.1 **线性模型基本形式**

 给定d个属性描述的示例 $\mathbf{x} = (x_{1};x_{2};...;x_{d})$，其中$x_{i}$是$\mathbf{x}$在第$i$个属性上的取值，**线性模型**(linear model)试图学得一个通过属性的线性组合来进行预测的函数，即

$$f(\mathbf{x}) = \omega_{1}x_{1} + \omega_{2}x_{2}\ +\ ...\ + \ \omega_{d}x_{d} + b \tag{1}$$
一般向量形式写成

$$f(\mathbf{x}) = \mathbf{\omega}^{T}\mathbf{x} + b \tag{2}$$
其中$\mathbf{\omega} = (\omega_{1};\omega_{2};...;\omega_{d})$

### 1.2 **线性回归**

1. 给定数据集$\mathbf{D}$ = {($\mathbf{x}_{1}$,$y_{1}$),($\mathbf{x}_{2}$,$y_{2}$),...,($\mathbf{x}_{m}$,$y_{m}$)}，其中 $\mathbf{x}_{i}$ = {$x_{i1}$;$x_{i2}$;...;$x_{id}$}，$y_{i}$ $\in$ $\mathbb{R}$
   线性回归问题(linear regression)试图学习模型
   $$f(\mathbf{x}) = \mathbf{\omega}^{T}\mathbf{x} + b$$
> 对离散属性，若属性值间存在“序”(order)关系，可通过连续化将其转化为连续值，例如二值属性“身高”的取值“高” “矮”可转化为{1.0, 0.0}，三值属性高度的取值“高” “中” “低”可转化为{1.0, 0.5, 0.0}；若属性间不存在序关系，假定有k个属性值，则通常转化为k维向量，例如“瓜类”的取值“西瓜” “南瓜” “黄瓜” 可转化为(0, 0, 1),(0, 1, 0),(1, 0, 0)
2. 使得$\ f(x_{i}) \simeq y_{i}$

> 该问题也被称作多**元线性回归**(multivariate linear regression)

2. 对于每个$x_{i}$，其预测值$y_{i} = \mathbf{\omega}^{T}\mathbf{x}_{i} + b$,采用均方误差（平方损失函数），则在训练集$\mathbf{D}$上，模型的损失函数为:
   $$L(f) = \sum_{n=1}^N(f(x_{i}) - y_{i})^{2} =\sum_{n=1}^N(y_{i} - \mathbf{\omega}^{T}\mathbf{x}_{i} - b)^{2} \tag{3}$$
   优化目标是损失函数最小化，即
   $$ (\mathbf{\omega}^{*}, b^{*}) = \underset{(\mathbf{\omega},b)}{arg\min}\sum_{n=1}^N(y_{i} - \mathbf{\omega}^{T}\mathbf{x}_i - b)^{2} \tag{4}$$

## 二、求解

- 可以用梯度下降法来求解上述最优化问题的数值解，但是实际上该最优化问题可以通过最小二乘法获得解析解。

- 为方便讨论，我们把$\mathbf{\omega}$和$b$吸收入向量形式$\mathbf{\omega} = (\mathbf{\omega};\mathbf{b})$，相应的把数据集$D$表示为一个$m\times(d+1)$大小的矩阵$\mathbf{X}$，其中每行对应于一个示例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1，即$$\mathbf{X} =  \begin{pmatrix}&x_{11}&x_{12}&\cdots&x_{1d}&1 \\ &x_{21}&x_{22}&\cdots &x_{2d}&1 \\ &\vdots&\vdots&\ddots&\vdots  &\vdots \\&x_{m1}&x_{m2}&\cdots &x_{md}&1 \end{pmatrix} =  \begin{pmatrix}&x^{T}_{1}&1 \\ &x^{T}_{2}&1 \\&\vdots&\vdots \\ &x^{T}_{m}&1\end{pmatrix} $$

  再把标记也写成向量形式$\mathbf{y} = (y_{1};y_{2};…;y_{m})$，类似于式(4)，有

  $$\hat{\mathbf{\omega}}^{*} = \underset{\hat{\mathbf{\omega}}}{arg\min}(\mathbf{y} - \mathbf{X}\mathbf{\hat{ \mathbf{\omega}}})^T(\mathbf{y} - \mathbf{X}\mathbf{\hat{\mathbf{\omega}}}) \tag{5}$$

  令$E_{\hat{\mathbf{\omega}}} = (\mathbf{y} - \mathbf{X}\mathbf{\hat{\mathbf{\omega}}})^T(\mathbf{y} - \mathbf{X}\mathbf{\hat{\mathbf{\omega}}})$，对$\hat{\omega}$求导得到

  $$\frac{\partial E_{\hat{\mathbf{\omega}}}}{\partial \hat{\mathbf{\omega}}} = 2\mathbf{X}^T(\mathbf{X\hat{\omega}}-y) \tag{6}$$

  令上式为0可得$\hat{\mathbf{\omega}}$最优解，但由于涉及矩阵逆的计算

- 当$\mathbf{X}^T\mathbf{X}$为**满秩矩阵**(full-rank matrix)或**正定矩阵**(positive definite matrix)时，令式(6)为0可得

  $$\hat{\mathbf{\omega}}^{*} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \tag{7}$$

  其中$(\mathbf{X}^T\mathbf{X})^{-1}$是矩阵$\mathbf{X}^T\mathbf{X}$的逆矩阵，最终学得的多元线性回归模型为

  $$f(x_{i}) = x_{i}^T（\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \tag{8}$$

- 当$\mathbf{X}^T\mathbf{X}$不是满秩矩阵，此时存在多个解析解，他们都能使得均方误差最小化。究竟选择哪个解作为输出，由算法的偏好决定。

  > 比如$N < n$（样本数量小于特征种类的数量），根据$\mathbf{X}$的秩小于等于$N,n$中的最小值，即小于等于$N$（矩阵的秩一定小于等于矩阵的行数和列数）； 而矩阵$\mathbf{X}^T\mathbf{X}$是$n\times n$大小的，它的秩一定小于等于$N$，因此不是满秩矩阵。

  常见的做法是引入正则化项

  - $L_{1}$正则化：此时称作Lasso Regression

    $$\hat{\mathbf{\omega}}^{*} = \underset{\hat{\mathbf{\omega}}}{arg\min}\left [ (\mathbf{y} - \mathbf{X}\mathbf{\hat{\omega}})^T(\mathbf{y} - \mathbf{X}\mathbf{\hat{\omega}})+ \lambda\left \|  \mathbf{\hat{\omega}}\right \|_{1}\right ] \tag{9}$$

  - $L_{2}$正则化：此时称作Ridge Regression

    $$\hat{\mathbf{\omega}}^{*} = \underset{\hat{\mathbf{\omega}}}{arg\min}\left [ (\mathbf{y} - \mathbf{X}\mathbf{\hat{\omega}})^T(\mathbf{y} - \mathbf{X}\mathbf{\hat{\omega}})+ \lambda\left \|  \mathbf{\hat{\omega}}\right \|_{2}^{2}\right ] \tag{10}$$

  - 同时包含$L_{1}$，$L_{2}$正则化，此时称作Elastic Net

    $$\hat{\mathbf{\omega}}^{*} = \underset{\hat{\mathbf{\omega}}}{arg\min}\left [ (\mathbf{y} - \mathbf{X}\mathbf{\hat  {\omega}})^T(\mathbf{y} - \mathbf{X}\mathbf{\hat{\omega}})+ \lambda\rho\left \|  \mathbf{\hat{\omega}}\right \|_{1} + \frac{\lambda(1-\rho)}{2}\left \|  \mathbf{\hat{\omega}}\right \|_{2}^{2}\right ] \tag{11}$$

    其中

    - $\lambda > 0$为正则化系数，调整正则化项与训练误差的比例
    - $0\leqslant\rho\leqslant1$ 为比例系数，调整$L_{1}$正则化与$L_{2}$正则化的比例

## 三、广义线性模型

为便于观察，我们把线性模型简写为

$$y = \mathbf{\omega}^{T}\mathbf{x} + b \tag{12}$$

如果将输出标记的对数作为线性模型逼近的目标，即

$$lny = \mathbf{\omega}^{T}\mathbf{x} + b \tag{13}$$

这就是**对数线性回归**(log-linear regression)

更一般的，考虑单调可微函数$g(·)$，令

$$y = g_{-1}(\mathbf{\omega}^{T}\mathbf{x} + b) \tag{14}$$

这样得到的模型称为**广义线性模型**(generalized-linear model)，其中函数$g(·)$称为**联系函数**(link function)。对数线性回归是广义线性模型在$g(·) = ln(·)$时的特例。

## 四、公式推导

对于(6)推导

$$\frac{\partial E_{\hat{\mathbf{\omega}}}}{\partial \hat{\mathbf{\omega}}} = 2\mathbf{X}^T(\mathbf{X\hat{\omega}}-y)$$

将$E_{\hat{\mathbf{\omega}}} = (\mathbf{y} - \mathbf{X}\mathbf{\hat{\omega}})^T(\mathbf{y} - \mathbf{X}\mathbf{\hat{\omega}})$展开可得

$$E_{\hat{\mathbf{\omega}}} = \mathbf{y}^T\mathbf{y} - \mathbf{y}^T\mathbf{X}\mathbf{\omega} - \mathbf{\omega}^T\mathbf{X}^T\mathbf{y} + \mathbf{\omega}^T\mathbf{X}^T\mathbf{X}\mathbf{\omega}$$

对$\hat{\omega}$求导可得

$$\frac{\partial E_{\hat{\mathbf{\omega}}}}{\partial \hat{\mathbf{\omega}}} = \frac{\partial  \mathbf{y}^T\mathbf{y}}{\partial \hat{\mathbf{\omega}}} - \frac{\partial \mathbf{y}^T\mathbf{X}\mathbf{\omega}}{\partial \hat{\mathbf{\omega}}} - \frac{\partial \mathbf{\omega}^T\mathbf{X}^T\mathbf{y}}{\partial \hat{\mathbf{\omega}}} + \frac{\partial \mathbf{\omega}^T\mathbf{X}^T\mathbf{X}\mathbf{\omega}}{\partial \hat{\mathbf{\omega}}}$$

由向量的求导公式可得

$$\frac{\partial E_{\hat{\mathbf{\omega}}}}{\partial \hat{\mathbf{\omega}}} = \mathbf{0} - \mathbf{X}^T\mathbf{y} - \mathbf{X}^T\mathbf{y} + (\mathbf{X}^T\mathbf{X} + \mathbf{X}\mathbf{X}^T)\hat{\mathbf{\omega}}$$

$$\frac{\partial E_{\hat{\mathbf{\omega}}}}{\partial \hat{\mathbf{\omega}}} = 2\mathbf{X}^T(\mathbf{X\hat{\omega}}-y)$$

> $\frac{\partial \mathbf{X}^T\mathbf{A}\mathbf{X}}{\partial \mathbf{X}} = (\mathbf{A} + \mathbf{A}^T)\mathbf{X}$

参考

- 《机器学习》
- [AI算法工程师手册](<http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/1_linear.html>)
- [南瓜书PumpkinBook](<https://datawhalechina.github.io/pumpkin-book/#/chapter3/chapter3>)
- [Matrix calculus](<https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities>)
