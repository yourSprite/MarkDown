# 决策树

**决策树**(decision tree)是一种常见的机器学习算法。以二分类任务为例，我们希望从给定训练数据集学得一个模型用以对新示例进行分类，这个把样本分类的任务，可看作对"当前样本属于正类吗？"这个问题的"决策"或"判定"过程。顾名思义，决策树是基于树结构来进行决策的，这恰是人类在面临决策问题时一种很自然的处理机制。

一般的，一颗决策树包含一个根节点、若干个内部节点和若干个叶节点；叶节点对应于决策结果，其他每个点则对应一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点包含样本全集。从根节点到每个叶节点的路径对应了一个判定测试序列。决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的**分而治之**(divide-and-conquer)策略。

决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：

1. 当前节点包含的样本全属于同一类别
2. 当前属性为空，或是所有样本在所有属性上取值相同
3. 当前节点包含的样本集合为空

决策树的优点为：可读性强，分类速度快

## 一、ID3

ID3名字中的ID是Iterative Dichotomiser(迭代二分器的简称)

ID3算法核心是在决策树的每个结点上应用信息增益准则选择特征，递归地构建决策树

- 从根结点开始，计算结点所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征划分出子结点

- 再对子结点递归地调用以上方法，构建决策树

- 直到所有特征的信息增益均很小或者没有特征可以选择为止，最后得到一个决策树 

  如果不设置特征信息增益的下限，则可能会使得每个叶子都只有一个样本点，从而划分得太细

### 1.1 信息增益

**信息熵**(information entropy)是度量样本集合**纯度**(purity)最常用的一种指标。假定当前样本集合$D$中第$k$类样本所占比例为$p_{k}(k= 1, 2,…,|y|)$，则$D$的信息熵定义为

$$Ent(D) = -\sum_{k=1}^{|y|}p_{k}log_{2}p_{k} \tag{1}$$

$Ent(D)$的值越小，则$D$的纯度越高

假定离散属性$a$有$V$个可能的取值$\left \{a^1,a^2,..,a^V  \right \}$，若使用$a$来对样本集$D$进行划分，则会产生$V$个分支节点，其中第$v$个分支节点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记为$D^v$。我们可根据式(1)计算出$D^v$的信息熵，再考虑到不同的分支节点所包含的样本数不同，给分支节点赋予权重$|D^v|/|D|$。即样本数越多的分支节点的影响越大，于是可计算出用属性$a$对样本集$D$进行划分所获得的**信息增益**(information gain)

$$Gain(D, a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v) \tag{2}$$

一般而言，信息增益越大，则意味着使用属性$a$来进行划分所获得的**纯度提升**越大。因此，我们可用信息增益来进行决策树的划分属性选择，及选择属性$a_* = \underset{a\in A}{arg\,max}\,Gain(D, a)$。著名的ID3决策树学习算法就是以信息增益为准则来选择划分属性。

### 1.2 算法步骤

1. 根据分类(忽略特征)计算信息熵
2. 根据特征分类计算信息增益
3. 选取最大信息增益的特征为划分属性
4. 迭代

## 二、C4.5

C4.5生成算法与ID算法相似，但是C4.5算法在生成过程中用信息增益比来选择特征

### 2.1 增益率

信息增益准则对可取值数目较多的属性有所偏好(如编号)，为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法不直接使用信息增益，而是使用**增益率**(gain ratio)来选择最优划分属性。采用与式(2)相同的符号表示，增益率定义为

$$Gain\_ratio = \frac{Gain(D, a)}{IV(a)} \tag{3}$$

其中

$$IV(a) = -\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2(\frac{|D^v|}{|D|}) \tag{4}$$

称为属性a的**固有值**(intrinsic value) 。属性a的可能取值数目越多(即$IV$越大),则$IV(a)$$的值通常会越大

需要注意的是，增益率准则对可取值数目较少的属性有所偏好，因此C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于水平的属性，再从中选择增益率最高的。

## 三、CART

CART是Classification and Regression Tree的简称，这是一种著名的决策树学习算法，分类和回归任务都可用

CART生成算法有两个生成准则：

- CART回归树：用平方误差最小化准则
- CART分类树：用基尼指数最小化准则

### 3.1 CART回归树

1. 一棵CART回归树对应着输入空间的一个划分，以及在划分单元上的输出值

   设输出$y$为连续变量，训练数据集$D = \left \{(x_1,y_1),(x_2,y_2),…,(x_n,y_n)    \right \}$

   设已经将输入空间划分为$M$个单元$R_1,R_2,…R_M$，且在每个单元$R_M$上有一个固定的输出值$c_m$。则CART 回归树模型  可以表示为：

   $$f(\mathbf x) = \sum_{m=1}^{M}c_m\mathbf I(\mathbf x \in R_m) \tag{5}$$

   其中$\mathbf I(·)$为指示性函数

2. 如果已知输入空间的单元划分，基于平方误差最小的准则，则CART回归树在训练数据集上的损失函数为：

   $$\sum_{m=1}^{M}\sum_{x \in R_m}^{}(y_i-c_m)^2 \tag{6}$$

   根据损失函数最小，则可以求解出每个单元上的最优输出值$c_m$为 :$R_m$上所有输入样本$x_i$对应的输出$y_i$的平均值

   即：$c_m = \frac{1}{N_m}\sum_{x_i \in R_m}^{}y_i$，其中$N_m$表示$R_m$中的样本数量

3. 定义$R_m$上样本的方差为$Var_m$，则有：$Var_m =\frac{1}{N_m} \sum_{x_i \in R_m}^{}(y_i-\widehat{c}_m)^2$，则CART回归树的损失函数重新写为：$N\times \sum_{m=1}^{M}(\frac{N_m}{N}\times Var_m)$，其中$N$为训练样本总数

   定义样本被划分到$R_m$中的概率为$P_m$，则$P_m = \frac{N_m}{N}$。由于$N$是常数，因此损失函数重写为

   $$\sum_{m=1}^{M}P_m\times Var_m \tag{7}$$

   其物理意义为：经过输入空间的单元划分之后，CART回归树的方差。通过这个方差来刻画CART回归树的纯度

4. 问题是输入空间的单元划分是未知的。如何对输入空间进行划分？

​       设输入$n$维：$\mathbf x = (x_1, x_2, …, x_n)^T$

- 选择第$j$维$x_j$和它的取值$s$作为切分变量和切分点。定义两个区域：

  $$R_1(j, s) = \left \{\mathbf x|x_j <= s  \right \}$$

  $$R_2(j, s) = \left \{\mathbf x|x_j >= s  \right \}$$

- 然后寻求最优切分变量$j$和最优切分点$s$。即求解：

  $$(j^*, s*) = \underset{(j, s)}{min}[\underset{c_1}{min}\sum_{x_i\in R_1(j, s) }^{}(y_i - c_1)^2 + \underset{c_2}{min}\sum_{x_i\in R_2(j, s) }^{}(y_i - c_1)^2] \tag{8}$$

  其意义为：

  - 首先假设已知切分变量$j$，则遍历切分点$s$，则有：

    $$c_1 = \frac{1}{N_1}\sum_{x \in R_1(j, s)}^{}y_i,\space\space\space\space\space c_2 = \frac{1}{N_2}\sum_{x \in R_2(j, s)}^{}y_i$$

    其中$N_1,N_2$代表样本数量

  - 然后遍历所有的特征维度，对每个维度找到最优切分点。从这些(切分维度,最优切分点)中找到使得损失函数最小的那个

5. 依次将输入空间划分为两个区域，然后重复对子区域划分，直到满足停止条件为止。这样的回归树称为最小二乘回归树

### 3.2 CART分类树

1. CART分类树采用**基尼指数**(Gini index)选择划分属性

2. 假设有$K$个分类，样本属于第$k$类的概率为$p_k = p(y=c_k)$。则概率分布的基尼指数为：

   $$Gini(D) = \sum_{k=1}^{K}p_k(1-p_k) = 1- \sum_{k=1}^{K}p_k^2 \tag{9}$$

   基尼指数表示：从数据集$D$中随机抽取两个样本，其类别标记不一致的概率。因此，基尼指数越小，则数据集的纯度越高

3. 对于给定的样本集合$D$，设属于类$c_k$的样本子集为$D_k$，则样本集的基尼指数为：

   $$Gini(D) = 1- \sum_{k=1}^{K}(\frac{N_k}{N})^2 \tag{10}$$

   其中$N$为样本总数，$N_k$为子集$D_k$的样本数量

4. 采用与式(2)相同的符号表示，属性a的基尼指数定义为

   $$Gini\_  index(D, a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v) \tag{11}$$

   于是，我们在候选属性集合$A$中，选择那个使得划分后基尼指数最小的属性作为最优划分属性，即$a_* = \underset{a\in A}{arg\,min}\,Gini\_index(D, a)$

## 四、剪枝处理

1. 决策树生成算法生成的树往往对于训练数据拟合很准确，但是对于未知的测试数据分类却没有那么准确。即出现过拟合现象

   过拟合产生得原因是决策树太复杂。解决的办法是：对决策树**剪枝**(pruning)，即对生成的决策树进行简化

2. 决策树的基本策略有**预剪枝**(prepruning)和**后剪枝**(post-pruning)

   **预剪枝**是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点

   **后剪枝**则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点

   剪枝的依据是：极小化决策树的整体损失函数或者代价函数

3. 决策树生成算法是学习局部的模型，决策树剪枝是学习整体的模型。即：生成算法仅考虑局部最优，而剪枝算法考虑全局最优

### 4.1 预剪枝

1. 预剪枝使得决策树很多分支都没有展开，这不仅降低了过拟合的风险还显著减少了决策树的训练时间开销和测试时间开销

2. 有些分支当前划分虽不能提升泛化性能，甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分确有可能导致性能显著提高

3. 预剪枝基于贪心本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险

### 4.2 后剪枝

1. 后剪枝决策树通常比预剪枝决策树保留了更多的分支
2. 一般情况下，后剪枝决策树的欠拟合风险很好，泛化性能往往优于预剪枝决策树
3. 后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对所有非叶节点进行注意考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多

## 五、连续与缺失值处理

### 5.1 连续值处理

由于连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可取值来对节点进行划分。可使用**分桶**(bucket)或**二分法**(bi-partition)对连续属性进行处理，C4.5决策树算法使用二分法

1. 现实学习任务中常常会遇到连续属性，此时可以使用连续属性离散化技术将连续特征转换为离散特征

2. 最常用的离散化技术为**二分法**(bi-partition)，4.5算法采取的方案

   - 给定样本集$D$和连续属性$a$，$a$在$D$上有$n$个不同的取值，将这些值从小到大排序，记为$\left \{a^1, a^2, … , a^n \right\}$

   - 基于划分点$t$可将$D$分为子集$D_t^-$和$D_t^+$，其中$D_t^-$包含哪些在属性a上取值不大于t的样本，而$D_t^+$则包含那些属性$a$上取值大于$t$的样本

   - 把区间$[a^i, a^{i+1})$的中位点$\frac{a^i+a^{i+1}}{2}$作为候选划分点，则包含n-1个元素的划分点集合为：

   $$T_a = \left\{\frac{a^i + a^{i+1}}{2}|1\leqslant i \leqslant n-1 \right \} \tag{12}$$

   - 然后像离散属性值一样来考察这些划分点，选取最优的划分点进行样本集合的划分，例如，对式(2)稍加改造

   $$\begin{equation}\begin{aligned}Gain(D, a) &= \underset{t \in T_a}{max}Gain(D, a, t) \\ &=  \underset{t \in T_a}{max}Ent(D) - \sum_{\lambda \in \left\{-, + \right\}}\frac{|D_t^\lambda|}{|D|}Ent(D_t^\lambda) \end{aligned}\end{equation}\tag{13}$$

   ​	  其中$Gain(D, a, t)$是样本集$D$基于划分点t二分后的信息增益

3. 事实上划分点的数量可以是任意正整数，划分点的位置也可以采取其它的形式

   事实上，划分点的数量、划分点的位置都是超参数，需要结合验证集、具体问题来具体分析

4. 与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性

### 5.2 缺失值处理

1. 现实任务中经常会遇到不完整样本，即样本的某些属性值缺失
   如果简单地放弃不完整样本，仅使用无缺失值的样本来进行学习，则是对数据信息的极大浪费
2. 这里有两个问题
   - 如何在属性值缺失的情况下选择划分属性
   - 定划分属性，如果样本在该属性上的值缺失，则如何划分样本

#### 5.2.1 划分属性选择

1. 给定训练集$D$和属性$a$，令$\widetilde D$表示在$D$中在属性$a$上没有缺失值的样本子集。则可以仅根据$\widetilde D$来判断属性$a$的优劣 。

2. 假设属性$a$有$V$个可取值$\left \{a^1, a^2, … , a^V \right\}$,令：

   - $\widetilde D^v$表示$\widetilde D$在属性$a$上取值为$a^v$的样本子集
   - $\widetilde D_k$表示$\widetilde D$中第$k$类样本子集(一共有$K$个分类)

   > 根据定义有：$\widetilde D = \bigcup_{k=1}^{K}\widetilde D_k = \bigcup_{v=1}^{V}\widetilde D^v$

3. 为每个样本$\mathbf x$赋予一个权重$\omega_x$，定义：

   $$\rho = \frac{\sum_{x \in \widetilde D}\omega_x}{\sum_{x \in D}\omega_x} \tag{14}$$

   $$\widetilde p_k = \frac{\sum_{x \in \widetilde D_k}\omega_x}{\sum_{x \in \widetilde D}\omega_x} \;\; (1\leqslant k\leqslant K) \tag{15}$$

   $$\widetilde r_v = \frac{\sum_{x \in \widetilde D_v}\omega_x}{\sum_{x \in \widetilde D}\omega_x}\;\; (1\leqslant v \leqslant V) \tag{16}$$

   其物理意义为：

   - $\rho$表示：无缺失值样本占总体样本的比例
   - $\widetilde p_k$表示：无缺失值样本中，第$k$类所占的比例

   - $\widetilde r_v$表示：无缺失值样本中，在属性$A$上取值为$a^v$的样本所占的比例

4. 基于上述定义将信息增益的计算式(2)推广为:

   $$\begin{equation}\begin{aligned}Gain(D, a) &= Gain(\widetilde D, a)\\ &=  \rho \times (Ent(\widetilde D) - \sum_{v=1}^{V}\widetilde r_vEnt(\widetilde D_v))\end{aligned}\end{equation}\tag{17}$$

#### 5.2.2 样本划分

1. 基于权重的样本划分：
   - 如果样本$x$在划分属性$A$上的取值已知，则：
     - 将$x$划入与其对应的子结点
     - $x$的权值在子结点中保持为$\omega _x$
   - 如果样本$x$在划分属性$A$上的取值未知，则：
     - 将$x$同时划入所有的子结点
     - $x$的权值在所有子结点中进行调整：在属性值为$a^v$对应的子结点中，该样本的权值调整为$\widetilde r_v \times \omega _x$
2. 直观地看，基于权重的样本划分就是让同一个样本以不同的概率分散到不同的子结点中去
   - 这一做法依赖于：每个样本拥有一个权重，然后权重在子结点中重新分配
   - C4.5算法使用了该方案

## 五、多变量决策树

1. 由于决策树使用平行于坐标轴的拆分，使得它对于一些很简单的问题很费力

   比如：当$x_2 > x_1$时为正类；否则为反类。这种拆分边界并不平行于坐标轴，使得决策树会用许多层的拆分来逼近这个边界

2. 解决方案是：**多变量决策树**(multivariate decision tree)

   - 传统的单变量决策树的分类边界每一段是与坐标轴平行的，每一段划分都直接对应了某个属性的取值
   - 多变量决策树的分类边界可以为斜线，它可以大大简化了决策树的模型

3. 多变量决策树中，每个非叶结点不再是针对某个属性，而是对属性的线性组合。即：每个非叶结点是一个形如$\sum_{i=1}^{d}\omega_ia_i = t$的线性分类器。其中：

   - $w_i$是$x_i$的权重
   - $d$为变量的数量
   - $t$表示这些变量的约束

4. 与传统的**单变量决策树**(univariate decision tree)不同，在多变量决策树的学习过程中，不是为每内部结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器

参考：

- 《机器学习》
- [AI算法工程师手册](<http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/1_linear.html>)



