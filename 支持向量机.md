# 支持向量机

1. 支持向量机(Support Vector Machine ，简称SVM）是一种二分类模型。它是定义在特征空间上的、间隔最大的线性分类器。

   - 间隔最大使得支持向量机有别于感知机。

     如果数据集是线性可分的，那么感知机获得的模型可能有很多个，而支持向量机选择的是间隔最大的那一个。

   - 支持向量机还支持核技巧，从而使它成为实质上的非线性分类器。

2. 支持向量机支持处理线性可分数据集、非线性可分数据集。

   - 当训练数据线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机（也称作硬间隔支持向量机）。
   - 当训练数据近似线性可分时，通过软间隔最大化，学习一个线性分类器，即线性支持向量机（也称为软间隔支持向量机）。
   - 当训练数据不可分时，通过使用核技巧以及软间隔最大化，学习一个非线性分类器，即非线性支持向量机。

3. 当输入空间为欧氏空间或离散集合、特征空间为希尔伯特空间时，将输入向量从输入空间映射到特征空间，得到特征向量。

   支持向量机的学习是在特征空间进行的。

   - 线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量。
   - 非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量。
     - 特征向量之间的内积就是核函数，使用核函数可以学习非线性支持向量机。
     - 非线性支持向量机等价于隐式的在高维的特征空间中学习线性支持向量机，这种方法称作核技巧。

4. 欧氏空间是有限维度的，希尔伯特空间为无穷维度的。

   - 欧式空间$\subseteq$希尔伯特空间$\subseteq$内积空间$\subseteq$赋范空间。

     - 欧式空间，具有很多美好的性质。

     - 若不局限于有限维度，就来到了希尔伯特空间。

       从有限到无限是一个质变，很多美好的性质消失了，一些非常有悖常识的现象会出现。

     - 如果再进一步去掉完备性，就来到了内积空间。

     - 如果再进一步去掉"角度"的概念，就来到了赋范空间。此时还有“长度”和“距离”的概念。

   - 越抽象的空间具有的性质越少，在这样的空间中能得到的结论就越少

   - 如果发现了赋范空间中的某些性质，那么前面那些空间也都具有这个性质。

## 一、间隔与支持向量

1. 给定训练样本集$D = \left \{(\mathbf x_1,y_1),(\mathbf x_2,y_2),…,(\mathbf x_m,y_m)  \right \},y_i\in \left \{-1,+1  \right \}$

   假设训练数据集是线性可分的，则学习的目标是基于训练集$D$在样本空间中找到一个划分超平面，将不同类别的样本分开。

   <img src="/Users/wangyutian/文档/markdown/pic/支持向量机/pic1.png" width = 500 height = 400 div align=center />

2. 能将训练样本分开的划分超平面可能有很多，而位于两个样本"正中间”的划分超平面对训练样本局部扰动的“容忍”性最好，即图6.1中间的超平面。换言之，这个划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强。

   划分超平面可通过如下线性方程来描述：
   $$
   \mathbf \omega^T\mathbf x+b=0 \tag{1}
   $$
   其中$\mathbf \omega = (\omega_1;\omega_1;…;\omega_d)$为法向量，决定了超平面的方向；$b$为位移项，决定了超平面与原点之间的距离。

3. 将超平面记为$(\mathbf \omega,b)$。样本空间任意点$\mathbf x$到超平面的距离可写为：
   $$
   r = \frac{|\mathbf \omega^T\mathbf x+b|}{||\mathbf \omega||} \tag{2}
   $$

   > $||·||$$:L_2$范数

   假设超平面$(\mathbf \omega,b)$能将训练样本正确分类，即对于$(\mathbf x_i,y_i) \in D$，若$y_i = +1$，则有$\mathbf \omega^T\mathbf x_i+b > 0$；若$y_i = -1$，则有$\mathbf \omega^T\mathbf x_i+b < 0$，令：
   $$
   \left\{\begin{matrix}
   \mathbf \omega^T\mathbf x_i+b \geqslant +1,\ y_i=+1\\ 
   \mathbf \omega^T\mathbf x_i+b \leqslant -1,\ y_i=-1
   \end{matrix}\right. \tag{3}
   $$
   如图6.2所示，距离超平面最近的这几个训练样本点使式(3)的等号成立，她们被称为**支持向量**(support vector)，两个异类支持向量到超平面的距离之和为：
   $$
   r = \frac{2}{||\mathbf \omega||} \tag{4}
   $$
   它被称为**间隔**(margin)。

   <img src="/Users/wangyutian/文档/markdown/pic/支持向量机/pic2.png" width = 500 height = 400 div align=center />

4. 欲找到具有**最大间隔**(maximum margin)的划分超平面，也就是要找到能满足式(3)中约束的参数$\mathbf \omega$和$b$，使得$\gamma$最大，即：
   $$
   \underset{\mathbf \omega,b}{max}\ \frac{2}{||\mathbf \omega||}\\
   s.t.\ y_i(\mathbf \omega^T\mathbf x_i+b)\geqslant 1,\ i=1,2,...,m \tag{5}
   $$
   最大化$||\mathbf \omega||^{-1}$等价于最小化$||\mathbf \omega||^2$，式(5)可重写为：
   $$
   \underset{\mathbf \omega,b}{min}\ \frac{1}{2}||\mathbf \omega||^2\\
   s.t.\ y_i(\mathbf \omega^T\mathbf x_i+b)\geqslant 1,\ i=1,2,...,m \tag{6}
   $$
   
   这就是支持向量机的基本型。

## 二、线性可分支持向量机

1. 我们希望求解式(6)来得到最大间隔划分超平面所对应的模型：
   $$
   f(\mathbf x) = \mathbf \omega^T\mathbf x+b \tag{7}
   $$
   其中$\mathbf \omega$和$b$是模型参数。式(6)是一个**凸二次规划**(quadratic programming)问题。

   > 凸优化问题 ，指约束最优化问题：
   > $$
   > \underset{\mathbf x}{min}\,f(\mathbf x) \\ s.t. h_i(\mathbf x)\leqslant 0\ (i=1, 2, …, m) \\ \ \ \ \ \ \ \,\,g_j(\mathbf x) = 0 \ (j=1, 2, …, n)
   > $$
   > 其中：
   >
   > - 目标函数$f(\mathbf x)$和约束函数$h_i(x)$都是$\mathbb R^n$上的连续可微的凸函数。
   > - 约束函数$g_j(\mathbf x)$是$\mathbb R^n$上的仿射函数。
   >
   > 当目标函数$f(\mathbf x)$是二次函数且约束函数$g_j(\mathbf x)$是仿射函数时，上述凸最优化问题成为凸二次规划问题。
   
2. 凸二次规划问题能直接用现成的优化计算包求解，但是用拉格朗日乘子法更高效。将线性可分支持向量机的最优化问题作为原始最优化问题，应用拉格朗日对偶性，通过求解**对偶问题**(dual problem)得到原始问题的最优解。这就是线性可分支持向量机的对偶算法。

   对偶算法的优点：

   - 对偶问题往往更容易求解。
   - 引入了核函数，进而推广到非线性分类问题。

3. 对式(6)的每条约束添加拉格朗日乘子$\alpha_i \geqslant 0$，则该问题的拉格朗日函数可写为：
   $$
   L(\mathbf \omega,b,\mathbf \alpha) = \frac{1}{2}||\mathbf \omega||^2 + \sum_{i=1}^{m}\alpha_i(1-y_i(\mathbf \omega^T\mathbf x + b)) \tag{8}
   $$
   

其中$\mathbf \alpha = (\alpha_1;\alpha_2;…;\alpha_m)$为拉格朗日乘子向量。

   - 根据拉格朗日对偶性，原始问题的解$\underset{\mathbf w,b}{min}\  \underset{\mathbf \alpha}{max}\ L(\mathbf \omega,b, \mathbf \alpha)$，对偶问题的解$\underset{\mathbf \alpha}{max}\ \underset{\mathbf w,b}{min}\ L(\mathbf \omega,b, \mathbf \alpha)$
   
   - 先求$\underset{\mathbf w,b}{min}\ L(\mathbf \omega,b, \mathbf \alpha)$。令$L(\mathbf \omega,b,\mathbf \alpha)$对$\mathbf \omega$和$b$的偏导为零可得：
     $$
     \mathbf \omega = \sum_{i=1}^{m}\alpha_iy_i\mathbf x_i \tag{9}
     $$
   
     $$
     0 = \sum_{i=1}^{m}\alpha_iy_i \tag{10}
     $$
   
   - 将式(9)和式(10)带入式(8)，得到式(6)的对偶问题：
     $$
     \underset{\mathbf \alpha}{max}\ \sum_{i=1}^{m}\alpha_i - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\mathbf x_{i}^{T}\mathbf x_j\\
     s.t.\ \sum_{i=1}^{m}\alpha_iy_i=0\\
     \alpha_i \geqslant 0,\ i=1,2,...,m
     \tag{11}
     $$
     解出$\mathbf \alpha$后，求出$\mathbf \omega$与$b$即可得到模型：

$$
   f(\mathbf x) = \mathbf \omega^T\mathbf x+b\\
   =\sum_{i=1}^{m}\alpha_iy_i\mathbf x_{i}^{T}\mathbf x+b\tag{12}
$$

4. 注意式(6)有不等式约束，因此上述过程满足KKT条件：
   $$
   \begin{cases} 
   		\alpha_i \geqslant 0\\ 
   		 y_if(\mathbf x_i)-1 \geqslant 0\\
   		 \alpha_i(y_if(\mathbf x_i)-1) = 0
   \end{cases}
   \tag{13}
   $$
   对任意训练样本$(\mathbf x_i, y_i)$，总有$\alpha_i=0$或$y_if(\mathbf x_i)=1$。若$\alpha_i=0$，则该样本将不会在式(12)的求和中出现，也就不会对$f(\mathbf x)$有任何影响；若$\alpha_i >0$，则必有$y_if(\mathbf x_i)=1$，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质：训练完成后，大部分的 训练样本都不需保留，最终模型仅与支持向量有关。

5. 求解式(11)是一个凸二次规划问题。这样的凸二次规划问题具有全局最优解，并且有多种算法可以用于这一问题的求解。

   当训练样本容量非常大时，这些算法往往非常低效。而序列最小最优化(Sequential Minimal Optimization:SMO）算法可以高效求解。

   - 输入：
     - 训练数据集$D = \left \{(\mathbf x_1,y_1),(\mathbf x_2,y_2),…,(\mathbf x_m,y_m)  \right \},y_i\in \left \{-1,+1  \right \}$
     - 精度$\varepsilon$

   - 输出：近似解$\hat{\mathbf \alpha}$

   - 算法步骤：

     - 取初值$\mathbf \alpha^{(0)} = 0,k=0$

     - 选取最优化变量$\alpha_{1}^{(k)},\alpha_{2}^{(k)}$，解析求解两个变量的最优化问题，求得最优解$\alpha_{1}^{(k+1)},\alpha_{2}^{(k+1)}$，更新$\mathbf \alpha$为$\mathbf \alpha^{(k+1)}$

     - 仅考虑$\alpha_{1},$和$\alpha_{2},$时，式(11)的约束可重写为$\alpha_{1}y_{1}+\alpha_{2}y_{2}=C$，其中$C=-\sum_{k\neq 1,2}\alpha_ky_k$是使$\sum_{i=1}^{m}\alpha_i y_i=0$成立的常数。

     - 若在精度$\varepsilon$范围内满足停机条件：
       $$
       \sum_{i=1}^{m}\alpha_i\tilde{y_i} = 0\\
       0\leqslant\alpha_i\leqslant C,\ i=1,2,...,m\\
       \tilde{y_i}f(\mathbf x_i) =
       \begin{cases} 
       		\geqslant 1,\ \alpha_i=0\\ 
       		 =1,\ 0<\alpha_i<C\\
       		 \leqslant 1,\ \alpha_i=C
       \end{cases}
       \tag{14}
       $$
       则退出迭代并令$\hat{\mathbf \alpha}=\mathbf \alpha^{(k+1)}$；否则令$k=k+1$，继续迭代。

       其中$f(\mathbf x_i)=\sum_{i=1}^{m}\alpha_iy_i\mathbf x_{i}^{T}\mathbf x+b$。
   
6. 对任意支持向量$(\mathbf x_s,y_s)$都有$y_sf(\mathbf x_s)=1$，即$y(\sum_{i\in S}\alpha_iy_i\mathbf x_i^T\mathbf x_s+b)=1$，S为所有支持向量的下标集。可得，$b=\frac{1}{|S|}\sum_{s\in S}(1/y_s-\sum_{i\in S}\alpha_iy_i\mathbf x_i^T\mathbf x_s)$


## 三 、非线性可分支持向量机

1. 前面假设训练样本是线性可分的，即存在一个划分超平面能正确划分两类样本的超平面。例如"亦或"问题就不是线性可分的。

   <img src="/Users/wangyutian/文档/markdown/pic/支持向量机/pic3.png" width = 800 height = 400 div align=center />

2. 对于这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间线性可分。如果原始空间是有限维，即属性有限，那么一定存在一个高维特征空间使样本可分。

3. 令$\phi(\mathbf x)$表示将$\mathbf x$映射后的特征向量，于是，在特征空间中划分超平面所对应的模型可表示为：
   $$
   f(\mathbf x) = \mathbf \omega^T\phi(\mathbf x)+b\tag{15}
   $$
   其中$\mathbf \omega$和$b$是模型参数。类似式(6)，有：
   $$
   \underset{\mathbf \omega,b}{min}\ \frac{1}{2}||\mathbf \omega||^2\\
   s.t.\ y_i(\mathbf \omega^T\phi(\mathbf x_i)+b)\geqslant 1,\ i=1,2,...,m \tag{16}
   $$
   其对偶问题是：
   $$
   \underset{\mathbf \alpha}{max}\ \sum_{i=1}^{m}\alpha_i - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\phi(\mathbf x_{i})^T\phi(\mathbf x_j)\\
   s.t.\ \sum_{i=1}^{m}\alpha_iy_i=0\\
   \alpha_i \geqslant 0,\ i=1,2,...,m\tag{17}
   $$

4. 求解式(17)涉及到计算$\phi(\mathbf x_{i})^T\phi(\mathbf x_j)$，这是样本$\mathbf x_i$与$\mathbf x_j$映射到特征空间之后的内积。由于特征空间维数可能很高，甚至无穷维，因此直接计算$\phi(\mathbf x_{i}^{T})\phi(\mathbf x_j)$通常是困难的。为了避开这个障碍，设想这样一个函数：
   $$
   \kappa(\mathbf x_i,\mathbf x_j)=\left \langle  \phi(\mathbf x_{i}),\phi(\mathbf x_j)\right \rangle=\phi(\mathbf x_{i})^T\phi(\mathbf x_j)\tag{18}
   $$
   $\mathbf x_i$与$\mathbf x_j$在特征空间的内积等于它们在原始样本空间中通过函数$\kappa(·,·)$计算的结果。于是式(17)可重写为：
   $$
   \underset{\mathbf \alpha}{max}\ \sum_{i=1}^{m}\alpha_i - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\kappa(\mathbf x_i,\mathbf x_j)\\
   s.t.\ \sum_{i=1}^{m}\alpha_iy_i=0\\
   \alpha_i \geqslant 0,\ i=1,2,...,m\tag{18}
   $$
   求解后得到：
   $$
   f(\mathbf x) = \mathbf \omega^T\phi(\mathbf x)+b\\
   =\sum_{i=1}^{m}\alpha_iy_i\phi(\mathbf x_{i})^T\phi(\mathbf x)+b\\
   =\sum_{i=1}^{m}\alpha_iy_i\kappa(\mathbf x,\mathbf x_i)+b\tag{19}
   $$
   这里的$\kappa(·,·)$就是**核函数**(kernel function)。式(19)显示出模型最优解可通过训练样本的 核函数展开，这一展示亦称**支持向量展式**(support vector expansion)。

5. 常用核函数：线性核、多项式核、高斯核、拉普拉斯核、Sigmoid核

   文本数据通常采用线性核，情况不明时可先尝试线性核。

   此外，还可通过函数组合得到：

   - 若$\kappa_1$和$\kappa_2$为核函数，则对于任意整数$\gamma_1,\gamma_2$，其线性组合：
     $$
     \gamma_1\kappa_1+\gamma_2\kappa_2\tag{20}
     $$
     也是核函数。

   - 若$\kappa_1$和$\kappa_2$为核函数，则核函数的直积：
     $$
     \kappa_1\otimes\kappa_2=\kappa_1(\mathbf x,\mathbf z)\kappa_2(\mathbf x,\mathbf z)\tag{21}
     $$
     也是核函数。

   - 若$\kappa_1$和$\kappa_2$为核函数，则对任意核函数$g(\mathbf x)$：
     $$
     \kappa(\mathbf x,\mathbf z)=g(\mathbf x)\kappa_1(\mathbf x,\mathbf z)g(\mathbf z)\tag{22}
     $$
   
6. 用线性分类方法求解非线性分类问题分两步：

   - 首先用一个变换将原空间的数据映射到新空间。
   - 再在新空间里用线性分类学习方法从训练数据中学习分类模型。

   这一策略称作核技巧。

## 四、软间隔与正则化

1. 在前面的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。然而，在显示任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分；即便恰好找到了谋个核函数是训练集在特征空间线性可分，也很难断定这个貌似线性可分的结果是不是由于过拟合造成的。

2. 缓解该问题的一个办法是允许支持向量机在一些样本上出错。为此，要引入**软间隔**(soft margin)的概念。

   <img src="/Users/wangyutian/文档/markdown/pic/支持向量机/pic4.png" width = 500 height = 400 div align=center />

3. 前面介绍的支持向量机形式是要求所有样本均满足约束(3)，即所有样本都必须正确划分，这称为**硬间隔**(hard margin)，而软间隔则是允许某些样本不满足约束：
   $$
   y_i(\mathbf \omega^T\mathbf x_i+b) \geqslant 1 \tag{23}
   $$
   
4. 在最大化间隔的同时，不满足约束的样本应尽可能少。于是，优化目标可写为：
   $$
   \underset{\mathbf \omega,b}{min}\ \frac{1}{2}||\mathbf \omega||^2+C\sum_{i=1}^{m}l_{0/1}(y_i(\mathbf \omega^T\mathbf x_i+b)-1) \tag{24}
   $$
   其中$C>0$是一个常数，$l_{0/1}$是"0/1损失函数"
   $$
   l_{0/1} =
   \begin{cases} 
   		1,\ if z < 0\\ 
   		0,\ otherwise
   \end{cases}
   \tag{25}
   $$
   C称作惩罚参数，一般由应用问题决定。

   - C值大时，对误分类的惩罚增大，此时误分类点凸显的更重要。
   - C值较大时，对误分类的惩罚增加，此时误分类点比较重要。
   - C值较小时，对误分类的惩罚减小，此时误分类点相对不重要。

   显然，当C为无穷大时，式(24)迫使所有样本均满足约束(23)，于是式(24)等价于式(6)；当C取有限值时，式(24)允许一些样本不满足约束。

5. $l_{0/1}$非凸、非连续，使得式(24)不易直接求解，于是，使用其他一些函数来替代$l_{0/1}$，称为**替代损失**(surrogate loss)。三种常用的替代损失函数为：

   hinge损失：$$l_{hinge}(z)=max(0,1-z)\tag{26}$$

   指数损失(exponential loss)：$$l_{exp}(z)=exp(-z)\tag{27}$$

   对率损失(logistic loss)：$$l_{log}(z)=log(1+exp(-z))\tag{28}$$

   <img src="/Users/wangyutian/文档/markdown/pic/支持向量机/pic5.png" width = 500 height = 400 div align=center />

6. 若采用hinge损失(合页函数)，式(24)变成：
   $$
   \underset{\mathbf \omega,b}{min}\ \frac{1}{2}||\mathbf \omega||^2+C\sum_{i=1}^{m}max(0,(y_i(\mathbf \omega^T\mathbf x_i+b)-1)) \tag{29}
   $$

7. 引入**松弛变量**(slack variable)$\xi_i\geqslant 0$，线性不可分的线性支持向量机的学习问题变成了凸二次规划问题，式(21)重写为：
   $$
   \underset{\mathbf \omega,b}{min}\ \frac{1}{2}||\mathbf \omega||^2+C\sum_{i=1}^{m}\xi_i\\
   s.t.\ y_i(\mathbf \omega^T\mathbf x_i+b) \geqslant 1-\xi_i\\
   \xi_i\geqslant0,\ i=1,2,...,m
   \tag{30}
   $$

   - 这称为线性支持向量机的原始问题。

   - 因为这是个凸二次规划问题，因此解存在。

     可以证明$\mathbf x$的解是唯一的；$b$的解不是唯一的，$b$的解存在于一个区间。

8. 对于给定的线性不可分的训练集数据，通过求解软间隔最大化问题得到的分离超平面为：$\mathbf \omega^T\mathbf x+b=0$以及对应的分类决策函数：$f(\mathbf x)=\mathbf \omega^T\mathbf x+b=0$，称之为线性支持向量机。

   - 线性支持向量机包含线性可分支持向量机。
   - 现实应用中训练数据集往往是线性不可分的，线性支持向量机具有更广泛的适用性。

9. 通过拉格朗日乘子法可得到式(30)的拉格朗日函数：
   $$
   L(\mathbf \omega,b,\mathbf \alpha,\mathbf \xi,\mathbf \mu)=\underset{\mathbf \omega,b}{min}\ \frac{1}{2}||\mathbf \omega||^2+C\sum_{i=1}^{m}\xi_i+\sum_{i=1}^{m}\alpha_i(1-\xi_i-y_i(\mathbf \omega^T\mathbf x_i+b))-\sum_{i=1}^{m}\mu_i\xi_i\tag{31}
   $$
   其中$\alpha_i\geqslant0,\mu\geqslant0$是拉格朗日乘子。

   令$L(\mathbf \omega,b,\mathbf \alpha,\mathbf \xi,\mathbf \mu)$对$\mathbf \omega,b,\xi_i$的偏导为零可得：
   $$
   \mathbf \omega=\sum_{i=1}^{m}\alpha_iy_i\mathbf x_i \tag{32}
   $$

   $$
   0=\sum_{i=1}^{m}\alpha_iy_i\tag{33}
   $$

   $$
   C=\alpha_i+\mu_i\tag{34}
   $$

   

10. 将式(32)-(34)带入式(31)可得到式(30)的对偶问题：
    $$
    \underset{\mathbf \alpha}{max}\ \sum_{i=1}^{m}\alpha_i - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\mathbf x_{i}^{T}\mathbf x_j\\
    s.t.\ \sum_{i=1}^{m}\alpha_iy_i=0\\
    0\leqslant\alpha_i \leqslant C,\ i=1,2,...,m
    \tag{35}
    $$
    
11. 上述过程满足KKT条件：
    $$
    \begin{cases} 
    		\alpha\geqslant0,\mu\geqslant0\\ 
    		y_if(\mathbf x_i)-1+\xi_i\geqslant0\\
    		\alpha_i(y_if(\mathbf x_i)-1+\xi_i)=0\\
    		\xi_i\geqslant0,\mu_i\geqslant0\tag{36}
    \end{cases}
    $$
    
12. 线性不可分的支持向量比线性可分时的情况复杂一些：

    - 若$\alpha_i<C$，则$\mu_i>0$， 则松弛量$\xi_i>0$。此时：支持向量恰好落在了间隔边界上。
    - 若$\alpha_i=C$， 则$\mu_i=0$，于是$\xi_i$可能为任何正数：
      - 若$0<\xi_i<1$，则支持向量落在间隔边界与分离超平面之间，分类正确。
      - 若$\xi_i=1$，则支持向量落在分离超平面上。
      - 若$\xi_i>1$，则支持向量落在分离超平面误分类一侧，分类错误。

## 五、支持向量回归

1. 对样本$(\mathbf x,y)$，传统回归模型通常直接基于模型输出$f(\mathbf x)$与真实输出$y$之间的 差别来计算损失，当且仅当$f(\mathbf x)$与$y$完全相同时，损失才为零。**支持向量回归**(Support Vector Regression，简称SVR)假设$f(\mathbf x)$与$y$之间最多有$\epsilon$的偏差，即仅当$f(\mathbf x)$与$y$之间的差别绝对值大于$\epsilon$时才计算损失。这相当于以$f(\mathbf x)$为中心，构建了一个宽度为$2\epsilon$的间隔带，若样本落入此间隔带，则被认为是预测正确的。

   <img src="/Users/wangyutian/文档/markdown/pic/支持向量机/pic6.png" width = 750 height = 400 div align=center />

2. SVR问题形式化为：
   $$
   \underset{\mathbf \omega,b}{min}\ \frac{1}{2}||\mathbf \omega||^2+C\sum_{i=1}^{m}l_\epsilon(f(\mathbf x)-y_i)\tag{37}
   $$
   其中$C$为正则化常数，$l_\epsilon$是如下图所示的$\epsilon$-不敏感损失($\epsilon$-insensitive loss)函数：
   $$
   l_\epsilon(z)=\begin{cases} 
   		0,\ \ \ \ \ \ \ \ if\,|z|\leqslant\epsilon\\ 
   		z-\epsilon,\ otherwise
   \end{cases}\tag{38}
   $$
   引入松弛变量$\xi_i$和$\hat \xi_i$，可将式(38)重写为：
   $$
   \underset{\mathbf \omega,b}{min}\ \frac{1}{2}||\mathbf \omega||^2+C\sum_{i=1}^{m}(\xi_i+\hat\xi_i)\\
   s.t.\ f(\mathbf x)-y_i\leqslant\epsilon+\xi_i,\\
   \ \ \ \ \ \ \ y_i-f(\mathbf x)\leqslant\epsilon+\hat\xi_i,\\
   \xi_i\geqslant0,\hat\xi_i\geqslant0,i=1,2,...,m
   \tag{39}
   $$

   > 间隔带两侧的松弛程度可有所不同

   <img src="/Users/wangyutian/文档/markdown/pic/支持向量机/pic7.png" width = 500 height = 400 div align=center />

3. 引入拉格朗日乘子$\mu_i\geqslant0,\hat\mu_i\geqslant0,\alpha_i\geqslant0,\hat\alpha_i\geqslant0$，得到拉格朗日函数：
   $$
   L(\mathbf \omega,b,\mathbf \alpha,\mathbf {\hat\alpha},\mathbf \xi,\mathbf {\hat\xi},\mathbf \mu,\mathbf {\hat\mu})\\
   =\frac{1}{2}||\mathbf \omega||^2+C\sum_{i=1}^{m}(\xi_i+\hat\xi_i)-\sum_{i=1}^{m}\mu_i\xi_i-\sum_{i=1}^{m}\hat\mu_i\hat\xi_i\\
   +\sum_{i=1}^{m}\alpha_i(f(\mathbf x)-y_i-\epsilon-\xi_i)+\sum_{i=1}^{m}\hat\alpha_i(y_i-f(\mathbf x)-\epsilon-\hat\xi_i)\tag{40}
   $$
   将式(7)代入，再令$L(\mathbf \omega,b,\mathbf \alpha,\mathbf {\hat\alpha},\mathbf \xi,\mathbf {\hat\xi},\mathbf \mu,\mathbf {\hat\mu})$对$\mathbf \omega,b,\xi_i,\hat\xi_i$偏导为零可得：
   $$
   \mathbf \omega=\sum_{i=1}^{m}(\hat\alpha_i-\alpha_i)\mathbf x_i\tag{41}
   $$

   $$
   0=\sum_{i=1}^{m}\hat\alpha_i-\alpha_i\tag{42}
   $$

   $$
   C=\alpha_i+\mu_i\tag{43}
   $$

   $$
   C=\hat\alpha_i+\hat\mu_i\tag{44}
   $$

   将式(41)-(44)代入式(40)，即可得到SVR的对偶问题：
   $$
   \underset{\mathbf \alpha,\mathbf{\hat\alpha}}{max}\ \sum_{i=1}^{m}y_i(\hat\alpha_i-\alpha_i)-\epsilon(\hat\alpha_i+\alpha_i)\\
    \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}(\hat\alpha_i-\alpha_i)(\hat\alpha_j-\alpha_j)\mathbf x_{i}^{T}\mathbf x_j\\
   s.t.\ \sum_{i=1}^{m}(\hat\alpha_i-\alpha_i)=0\\
   0\leqslant\alpha_i, \hat\alpha_i\leqslant C\tag{45}
   $$
   上述过程满足KKT条件：
   $$
   \begin{cases} 
   		\alpha_i(f(\mathbf x)-y_i-\epsilon-\xi_i)=0\\ 
   		\hat\alpha_i(y_i-f(\mathbf x)-\epsilon-\hat\xi_i)=0\\
   		\alpha_i\hat\alpha_i=0,\mu_i\hat{\mu_i}=0\\
   		(C-\alpha_i)\xi_i=0,(C-\hat\alpha_i)\hat\xi_i=0\tag{46}
   \end{cases}
   $$
   
4. 将式(41)带入式(7)，SVR的解形如：
   $$
   f(\mathbf x)=\sum_{i=1}^{m}(\hat\alpha_i-\alpha_i)\mathbf x_{i}^{T}\mathbf x+b\tag{47}
   $$
   使$(\hat\alpha_i-\alpha_i)\neq0$的样本即为SVR的支持向量，它们必落在$\epsilon$-间隔带之外。SVR的支持向量仅是训练样本的一部分，即其解仍具有稀疏性。

5. 由KKT条件可以看出，对每个样本$(\mathbf x_i,y_i)$都有$(C-\alpha_i)\xi_i=0$且$\alpha_i(f(\mathbf x_i)-y_i-\epsilon-\xi_i)=0$。于是，在得到$\alpha_i$后，若$0<\alpha_i<C$，则必有$\xi_i=0$，进而有：
   $$
   b=y_i+\epsilon-\sum_{j=1}^{m}((\hat\alpha_j-\alpha_j)\mathbf x_{j}^{T}\mathbf x_i)\tag{48}
   $$
   可以选任意满足$0<\alpha_i<C$的样本求得$b$，或选取多个(或所有)满足$0<\alpha_i<C$的样本求得$b$后取平均值
   
6. 若考虑到特征映射，则相应的式(41)将形如：
   $$
   \mathbf \omega=\sum_{i=1}^{m}(\hat\alpha_i-\alpha_i)\phi(\mathbf x_i)\tag{49}
   $$
   则SVR可表示为：
   $$
   f(\mathbf x)=\sum_{i=1}^{m}(\hat\alpha_i-\alpha_i)\kappa(\mathbf x,\mathbf x_i)+b\tag{47}
   $$
   其中$\kappa(\mathbf x,\mathbf x_i)=\phi(\mathbf x_i)^T\phi(\mathbf x_i)$

## 六、其他讨论

1. 支持向量机的优点：

   - 有严格的数学理论支持，可解释性强。
   - 能找出对任务至关重要的关键样本（即：支持向量）。
   - 采用核技巧之后，可以处理非线性分类/回归任务。

2. 支持向量机的缺点：

   - 训练时间长。当采用SMO算法时，由于每次都需要挑选一对参数，因此时间复杂度为$O(N^2)$，其中$N$为$\mathbf \alpha$的长度，也就是训练样本的数量。
   - 当采用核技巧时，如果需要存储核矩阵，则空间复杂度为$O(N^2)$。
   - 模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。

   因此支持向量机目前只适合小批量样本的任务，无法适应百万甚至上亿样本的任务。

## 七、公式推导

1. 式(3)

   假设超平面是$(\mathbf \omega')^T\mathbf x+b'=0$，对于$(\mathbf x_i,y_i)\in D$有：
   $$
   \left\{\begin{matrix}
   (\mathbf \omega')^T\mathbf x_i+b'>0,\ y_i=+1\\ 
   (\mathbf \omega')^T\mathbf x_i+b'<0,\ y_i=-1
   \end{matrix}\right.
   $$
   根据几何间隔，上述关系修正为：
   $$
   \left\{\begin{matrix}
   (\mathbf \omega')^T\mathbf x_i+b'\geqslant+\zeta,\ y_i=+1\\ 
   (\mathbf \omega')^T\mathbf x_i+b'\leqslant-\zeta,\ y_i=-1
   \end{matrix}\right.
   $$
   其中$\zeta$为某个大于零的常数，两边同时除以$\zeta$，在此修正上述关系为：
   $$
   \left\{\begin{matrix}
   (\frac{1}{\zeta}\mathbf \omega')^T\mathbf x_i+\frac{b'}{\zeta}\geqslant+1,\ y_i=+1\\ 
   (\frac{1}{\zeta}\mathbf \omega')^T\mathbf x_i+\frac{b'}{\zeta}\leqslant-1,\ y_i=-1
   \end{matrix}\right.
   $$
   令$\mathbf \omega = \frac{1}{\zeta}\mathbf \omega'$，$b=\frac{b'}{\zeta}$，则以上关系可写为：
   $$
   \left\{\begin{matrix}
   \mathbf \omega^T\mathbf x_i+b \geqslant +1,\ y_i=+1\\ 
   \mathbf \omega^T\mathbf x_i+b \leqslant -1,\ y_i=-1
   \end{matrix}\right.
   $$
   
2. 式(11)

   式(9)带入式(8)：
   $$
   \underset{\mathbf w,b}{min}\ L(\mathbf \omega,b, \mathbf \alpha)=\frac{1}{2}\mathbf \omega^T\mathbf \omega+\sum_{i=1}^{m}\alpha_i-\sum_{i=1}^{m}\alpha_i y_i\mathbf \omega^T\mathbf x_i-\sum_{i=1}^{m}\alpha_iy_ib\\
   \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ =\frac{1}{2}\mathbf \omega^T\sum_{i=1}^{m}\alpha_i y_i\mathbf x_i-\mathbf \omega^T\sum_{i=1}^{m}\alpha_i y_i\mathbf x_i+\sum_{i=1}^{m}\alpha_i-b\sum_{i=1}^{m}\alpha_i y_i\\
   \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =-\frac{1}{2}\mathbf \omega^T\sum_{i=1}^{m}\alpha_i y_i\mathbf x_i+\sum_{i=1}^{m}\alpha_i-b\sum_{i=1}^{m}\alpha_i y_i
   $$
   又$\sum_{i=1}^{m}\alpha_iy_i=0$，所以上式最后一项可化为0，于是得：
   $$
   \underset{\mathbf w,b}{min}\ L(\mathbf \omega,b, \mathbf \alpha)=-\frac{1}{2}\mathbf \omega^T\sum_{i=1}^{m}\alpha_i y_i\mathbf x_i+\sum_{i=1}^{m}\alpha_i\\
   \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =-\frac{1}{2}(\sum_{i=1}^{m}\alpha_i y_i\mathbf x_i)^T\sum_{i=1}^{m}\alpha_i y_i\mathbf x_i+\sum_{i=1}^{m}\alpha_i\\
   \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =-\frac{1}{2}\sum_{i=1}^{m}\alpha_i y_i\mathbf x_i^T\sum_{i=1}^{m}\alpha_i y_i\mathbf x_i+\sum_{i=1}^{m}\alpha_i\\
   \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ =\sum_{i=1}^{m}\alpha_i - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\mathbf x_{i}^{T}\mathbf x_j
   $$
   所以：
   $$
   \underset{\mathbf \alpha}{max}\ \underset{\mathbf w,b}{min}\ L(\mathbf \omega,b, \mathbf \alpha)=\underset{\mathbf \alpha}{max}\ \sum_{i=1}^{m}\alpha_i - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\mathbf x_{i}^{T}\mathbf x_j\\
   $$
   