# 约束优化-拉格朗日乘子法

**拉格朗日乘子法**(Lagrange multipliers)是一种寻找多元函数在一组约束下的极值方法。通过引入拉格朗日乘子，可将有$d$个变量与$k$个约束条件的最优化问题转化为具有$d+k$个变量的无约束优化问题求解

## 一、原始问题

1. 假设$\mathbf x$为$d$维向量，，欲寻找$\mathbf x$的某个取值$\mathbf x^*$，使目标函数$f(\mathbf x)$最小且满足$m$个等式约束和$n$个不等式约束，且可行域$\mathbb D \subset \mathbb R^d$非空的优化问题

   $$\underset{\mathbf x}{min}\,f(\mathbf x) \\ s.t. h_i(\mathbf x)\leqslant 0\ (i=1, 2, …, m) \\ \ \ \ \ \ \ \,\,g_j(\mathbf x) = 0 \ (j=1, 2, …, n) \tag{1}$$

2. 引入拉格朗日乘子$\alpha = (\alpha_1, \alpha_2, …, \alpha_m)^T$和$\beta = (\beta_1, \beta_2, …, \beta_n)^T$，相应的拉格朗日函数为

   $$L(\mathbf x,\mathbf \alpha,\mathbf \beta) = f(\mathbf x) + \sum_{i=1}^{m}\ \alpha_i h_i(\mathbf x) + \sum_{j=1}^{n} \beta_j g_j(\mathbf x) \tag{2}$$

   这里$\mathbf x = (x_1, x_2, …,x_d)^T \in \mathbb R^d$，$\alpha_i \geqslant 0$

   假设$f(\mathbf x)$，$h_i(\mathbf x)$，$g_j(\mathbf x)$是定义在$\mathbb R^d$上的连续可微函数

   $L(\mathbf x,\mathbf \alpha,\mathbf \beta)$是$\mathbf x,\mathbf \alpha,\mathbf \beta$的多元非线性函数

3. 定义函数：

   $$\theta_P(\mathbf x) = \underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\ L(\mathbf x, \mathbf \alpha, \mathbf \beta) \tag{3}$$

   其中下标$P$表示原始问题。则有：

   $$
   \theta_P(\mathbf x) =
   \begin{cases} 
   		f(\mathbf x),\ if \ \mathbf x\ statisfy\ original\ problem's\  constraint \\ 
   		 +\infty, \ or\ else \tag{4}
   \end{cases}
   $$

   - 若$\mathbf x$ 满足原问题的约束，则很容易证明$L(\mathbf x,\alpha,\mathbf \beta) = f(\mathbf x) + \sum_{i=1}^{m} \alpha_i h_i(\mathbf x) \leqslant f(\mathbf x) $，等号在$\alpha_i = 0$时成立

   - 若$\mathbf x$ 不满足原问题的约束：

     - 若不满足$h_i(\mathbf x) \leqslant 0$：设违反的为$h_{i}(\mathbf x) > 0$，则令$ \alpha_i → \infty$，有：

       $L(\mathbf x,\mathbf \alpha,\mathbf \beta) = f(\mathbf x) + \sum_{i=1}^{m}\mathbf \alpha_i h_i(\mathbf x) → \infty$

     - 若不满足$g_j(\mathbf x) = 0$：设违反的为$g_j(\mathbf x) \neq 0$，则令$\beta_j g_j(\mathbf x) → \infty$，有：

       $L(\mathbf x,\mathbf \alpha,\mathbf \beta) = f(\mathbf x) + \sum_{i=1}^{m}\ \alpha_i h_i(\mathbf x) + \sum_{j=1}^{n} \beta_j g_j(\mathbf x) → \infty$

4. 考虑极小化问题：

   $$\underset{\mathbf x}{min} \, \theta_P(\mathbf x) = \underset{\mathbf x}{min}  \underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\ L(\mathbf x, \mathbf \alpha, \mathbf \beta) \tag{5}$$

   则该问题与原始最优化问题是等价的，即他们有相同的问题

   - $\underset{\mathbf x}{min}  \underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\ L(\mathbf x, \mathbf \alpha, \mathbf \beta) $称为广义拉格朗日函数的极大极小问题
   - 为了方便讨论定义原始问题的最优值为：$p^* = \underset{\mathbf x}{min} \, \theta_P(\mathbf x)$

## 二、对偶问题

1. 定义$\theta_D (\mathbf \alpha, \mathbf \beta)= \underset{\mathbf x}{min}\, L(\mathbf x, \mathbf \alpha, \mathbf \beta)$，考虑极大化$\theta_D (\mathbf \alpha, \mathbf \beta)$，即：

   $$\underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\,\theta_D (\mathbf \alpha, \mathbf \beta) = \underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\, \underset{\mathbf x}{min}\, L(\mathbf x, \mathbf \alpha, \mathbf \beta) \tag{6}$$

   问题$\underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\, \underset{\mathbf x}{min}\, L(\mathbf x, \mathbf \alpha, \mathbf \beta)$称为广义拉格朗日函数的极大极小问题。它可以表述为约束最优化问题：

   $$\underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\,\theta_D (\mathbf \alpha, \mathbf \beta) = \underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\, \underset{\mathbf x}{min}\, L(\mathbf x, \mathbf \alpha, \mathbf \beta) \\ s.t. \alpha_i \geqslant 0,i=1,2,..,k \tag{7}$$

   称为原始问题的对偶问题。

   为了方便讨论，定义对偶问题的最优值为：$d^* = \underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\,\theta_D (\mathbf \alpha, \mathbf \beta) $

2. 定理一：若原问题和对偶问题具有最优值，则：

   $$d^* = \underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\, \underset{\mathbf x}{min}\, L(\mathbf x, \mathbf \alpha, \mathbf \beta) \leqslant \underset{\mathbf x}{min}  \underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\ L(\mathbf x, \mathbf \alpha, \mathbf \beta) = p^* \tag{8}$$

   - 推论一：设$\mathbf x^*$为原始问题的可行解，且$\theta_P(\mathbf x^*)$的值为$p^*$；$\mathbf \alpha^*,\mathbf \beta^*$为对偶问题的可行解，$\theta_D (\mathbf \alpha^*, \mathbf \beta^*)$值为$d^*$。

     如果有$p^* = d^*$，则$\mathbf x^*,\mathbf \alpha^*,\mathbf \beta^*$分别为原始问题和对偶问题的最优解

3. 定理二：假设函数$f(\mathbf x)$和$h_i(\mathbf x)$为凸函数，$g_j(\mathbf x)$ 是仿射函数；并且假设不等式约束$h_i(\mathbf x)$是严格可行的，即存在$\mathbf x$，对于所有$i$有$h_i(\mathbf x) < 0$。则存在$\mathbf x^*,\mathbf \alpha^*,\mathbf \beta^*$，使得：$\mathbf x$是原始问题$\underset{\mathbf x}{min} \, \theta_P(\mathbf x)$的解，$\mathbf \alpha^*,\mathbf \beta^*$是对偶问题$\underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\,\theta_D (\mathbf \alpha, \mathbf \beta)$的解，并且$p^*=d^* = L(\mathbf x^*,\mathbf \alpha^*,\mathbf \beta^*)$

4. 定理三：假设函数$f(\mathbf x)$和$h_i(\mathbf x)$为凸函数，$g_j(\mathbf x)$ 是仿射函数；并且假设不等式约束$h_i(\mathbf x)$是严格可行的，即存在$\mathbf x$，对于所有$i$有$h_i(\mathbf x) < 0$。则存在$\mathbf x^*,\mathbf \alpha^*,\mathbf \beta^*$，使得：$\mathbf x$是原始问题$\underset{\mathbf x}{min} \, \theta_P(\mathbf x)$的解，$\mathbf \alpha^*,\mathbf \beta^*$是对偶问题$\underset{\mathbf \alpha,\mathbf \beta:\alpha_i\geqslant 0}{max}\,\theta_D (\mathbf \alpha, \mathbf \beta)$的解的充要条件是：$\mathbf x^*,\mathbf \alpha^*,\mathbf \beta^*$满足下面的**Karush-kuhn-Tucker(KKT)**条件：
   $$
   \nabla_\mathbf x = L(\mathbf x^*,\mathbf \alpha^*,\mathbf \beta^*) \\
   \nabla_\mathbf \alpha = L(\mathbf x^*,\mathbf \alpha^*,\mathbf \beta^*) \\
   \nabla_\mathbf \beta = L(\mathbf x^*,\mathbf \alpha^*,\mathbf \beta^*) \\
   \alpha_i^* h_i^*(\mathbf x^*) = 0,i=1,2,...,k \\
   h_i^*(\mathbf x^*) \leqslant 0 ,i=1,2,...,k\\
   \alpha_i^* \geqslant 0 ,i=1,2,...,k \\
   g_j^*(\mathbf x^*) = 0,j=1,2,...,k
   $$
   
5. 仿射函数：仿射函数即由**1**阶多项式构成的函数。

   一般形式为:$f(\mathbf x) = \mathbf A \mathbf x + b$。这里：$\mathbf A$是一个$m \times n$矩阵，$\mathbf x$是一个$k$维列向量，$b$是一个$m$维列向量，它实际上反映了一种从$k$维到$m$维的空间线性映射关系。

6. 凸函数：设$f$为定义在区间$\mathcal{X}$上的函数，若对$\mathcal{X}$上的任意两点$\mathbf x_1,\mathbf x_2$和任意的实数$\lambda \in (0, 1)$ ，总有$f(\lambda\mathbf x_1 + (1-\lambda)\mathbf x_2) \geqslant \lambda f(\mathbf x_1) + (1-\lambda)f(\mathbf x_2)$，则$f$称为$\mathcal{X}$上的凸函数 。

参考：

- 《机器学习》
- [AI算法工程师手册](<http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/1_linear.html>)