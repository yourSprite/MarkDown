# 朴素贝叶斯

## 一、贝叶斯定理

1. 设$\mathbb{S}$为试验$E$的样本空间；  $B_1,B_2,…,B_n$为$E$的一组事件。若 ：

   - $B_i\bigcap B_j = \phi,i\neq j,i,j=1, 2,…,n$
   - $B_1\bigcup B_2…\bigcup B_n = \mathbb S$

   则称$B_1,B_2,…,B_n$为样本空间$\mathbb S$的一个划分

2. 如果$B_1,B_2,…,B_n$为样本空间$\mathbb S$的一个划分，则对于每次试验，事件$B_1,B_2,…,B_n$中有且仅有一个事件发生

3. 全概率公式：设试验$E$的样本空间为$\mathbb S$，$A$为$E$的事件，$B_1,B_2,…,B_n$为样本空间$\mathbb S$的一个划分，且$P(B_i) \geqslant 0(i = 1, 2, …, n)$，则有：

   $$P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2)+…+P(A|B_n)P(B_n) = \sum_{j=1}^{n}P(A|B_i)P(B_i) \tag{1}$$

4. 贝叶斯定理：设试验$E$的样本空间为$\mathbb S$，$A$为$E$的事件，$B_1,B_2,…,B_n$为样本空间$\mathbb S$的一个划分，且$P(A) > 0,P(B_i) \geqslant 0(i = 1, 2, …, n)$，则有：

   $$P(B_i|A) = \frac {P(A|B_i)P(B_i)} {\sum_{j=1}^{n}P(A|B_j)P(B_j)}=  \frac {P(A|B_i)P(B_i)} {P(A)}\tag{2}$$

## 二、贝叶斯决策论

**贝叶斯决策论**(Bayesian decision theroy)是概率框架下实施决策的基本方法。对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。

1. 先验概率：根据以往经验和分析得到的概率

   后验概率：根据已经发生的事件来分析得到的概率

   例：假设山洞中有熊出现的事件为$Y$，山洞中传来一阵熊吼的事件为$X$

   - 山洞中有熊的概率为$P(X)$。它是先验概率，根据以往的数据分析或者经验得到的概率
   - 听到熊吼之后认为山洞中有熊的概率为$P(X|Y)$。它是后验概率，得到本次试验的信息从而重新修正的概率

2. 假设有$N$中可能的类别标记，即$Y = \left\{c_1, c_2, …, c_n \right\}$，$\lambda_{ij}$是将一个真实标记为$c_j$误分类为$c_i$所产生的损失，基于后验概率$P(c_i|\mathbf x)$可获得将样本$\mathbf x$分类为$c_i$所产生的期望损失(excepted loss)，即在样本$\mathbf x$上的**条件风险**(condition risk)

   $$R(c_i|\mathbf x) = \sum_{j=1}^{N}\lambda_{ij}P(c_j|x) \tag{3}$$

   > 决策论中将**期望损失**称为**风险**(risk)

3. 我们的任务是寻找一个判定准则$h:x→y$以最小化总体风险

   $$R(h) = \mathbb E[R(h(\mathbf x))|\mathbf x] \tag{4}$$

4. 显然，对每个样本$\mathbf x$，若$h$能最小化条件风险$R(h(\mathbf x)|\mathbf x)$，则总体风险$R(h)$也将被最小化。这就产生了**贝叶斯判定准则**(Bayes decision rule)：为最小化总体风险，只需在每个样本上选择哪个能使条件风险$R(c|\mathbf x)$最小的类别标记，即：

   $$h^*(\mathbf x) = \underset{c\in y}{arg\, min}\,R(c|\mathbf x) \tag{5}$$

   此时，$h^*$称为**贝叶斯最优分类器**(Bayes optimal classifier)，与之对应的总体风险$R(h^*)$称为**贝叶斯风险**。$1-R(h*)$反应了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限

5. 若目标是最小化分类错误率，则误判损失$\lambda_{ij}$可写为：

   $$\lambda_{ij} = \left\{\begin{matrix}0,\;if i = j\\ 1,\;otherwise\end{matrix}\right. \tag{6}$$

   此时，条件风险：

   $$R(c|\mathbf x) = 1- P(c|\mathbf x) \tag{7}$$

   > 式(7)推导:
   >
   > 由式(3)和式(6)可得：
   > $$
   > R(c_i|\mathbf x) = 1*P(c_1|\mathbf x) + 1*P(c_2|\mathbf x) + ... + 0*1*P(c_i|\mathbf x)+...+ 1*P(c_N|\mathbf x)
   > $$
   > 又$\sum_{j=1}^{N}P(c_i|\mathbf x) = 1$，则：
   > $$
   > R(c|\mathbf x) = 1- P(c|\mathbf x)
   > $$

   于是，最小化分类错误率的贝叶斯最优分类器为

   $$h^*(\mathbf x) = \underset{c\in y}{arg\, max}\,P(c|\mathbf x) \tag{8}$$

   即对每个样本$\mathbf x$，选择能使后验概率$P(c|\mathbf x)$最大的类别标记

6. 给定$\mathbf x$，估计后验概率$P(c|\mathbf x)$有两种策略：

   - 通过直接建模$P(c|\mathbf x)$来预测$c$，这样得到的是**判别式模型**(descriminative models)，如决策树、BP神经网络、支持向量机等
   - 先对联合概率分布$P(\mathbf x, c)$建模，然后由此获得$P(c|\mathbf x)$，这样得到的是**生成式模型**(generative models)，如朴素贝叶斯

7. 对生成式模型，必然考虑

   $$P(c|\mathbf x) = \frac{P(\mathbf x, c)}{P(\mathbf x)} \tag{9}$$

   基于贝叶斯定理

   $$P(c|\mathbf x) = \frac{P(c)P(\mathbf x|c)}{P(\mathbf x)} \tag{10}$$

   其中，$P(c)$是类**先验**(prior)概率，$P(c|\mathbf x)$是样本$x$相对于类标记$c$的**类条件概率**(class-conditional probability)，或称为**似然**(likelihood)，$P(x)$是用于归一化的**证据**(evidence)因子

8. 对给定样本$\mathbf x$，证据因子$P(x)$与类标记无关，因此$P(\mathbf x|c)$的问题就转化为如何基于训练数据$D$来估计先验$P(c)$和似然$P(x|c)$

## 三、极大似然估计

1. 假设$P(x|c)$具有确定形式且被参数向量$\mathbf {\theta}_c$唯一确定，并将$P(x|c)$记为$P(x|\mathbf \theta_c)$

2. 概率模型的训练过程就是**参数估计**(parameter estimation)过程，统计学界有两种不同解决方案

   - **频率注意学派**(Frequentist)认为参数虽然未知，但确是客观存在的固定值，可通过优化似然函数等准则来确定参数值
   - **贝叶斯学派**(Bayesian)认为参数是未观察到的随机变量，其本身也可由分布。可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布

3. 基于频率主义学派的**极大似然估计**(Maximum Likelihood Estimation，简称MLE)，这是根据数据采样来估计概率分布参数的经典方法

   令$D_c$表示训练集$D$中第$c$类样本组成的集合，假设这些样本是独立同分布的，则参数$\mathbf \theta_c$对于数据集$D_c$的似然是

   $$P(D_c|\mathbf \theta_c) = \prod_{x\in D_c}P(\mathbf x | \mathbf c) \tag{11}$$

4. 对$\mathbf \theta_c$进行极大似然估计，式(10)连乘操作易造成下溢，通常使用对数似然(log-likelihood)

   $$LL(\mathbf \theta_c) = logP(D_c|\mathbf \theta_c) = \sum_{\mathbf x \in D_c}logP(\mathbf x|\mathbf \theta_c) \tag{12}$$

   此时参数

   $$\hat {\mathbf \theta_c} = \underset{\theta_c}{arg\, min}\,LL(\mathbf \theta_c) \tag{13}$$

## 四、朴素贝叶斯分类器

1. **朴素贝叶斯分类器**(naive Bayes classifier)采用了**属性条件独立性假设**(attribute conditional independence assumption)：对已知类别，假设所有属性相互独立

   > 基于有限训练样本直接估计联合概率，在计算上将会遭遇组合爆炸问题，在数据上将会遇到样本稀疏问题；属性越多，问题越严重

   基于属性条件独立性假设，式(10)可重写为

   $$P(c|\mathbf x) = \frac{P(c)P(\mathbf x|c)}{P(\mathbf x)} = \frac{P(c)}{P(\mathbf x)}\prod_{i=1}^{d}P(x_i|c) \tag{14}$$

   其中$d$为属性数目，$x_i$为$\mathbf x$在第$i$个属性上的取值

2. 对于所有类别来说$P(\mathbf x)$相同，因此基于式(8)贝叶斯判定准则有

   $$h_{nb}(\mathbf x) = \underset{c\in y}{arg\, max}\,\prod_{i=1}^{d}P(x_i|c) \tag{15}$$

   这就是朴素贝叶斯分类器的表达式

   > 朴素贝叶斯分类器的训练过程就是基于训练集$D$来估计类先验概率$P(c)$，并为每个属性 估计条件概率$P(x_i|c)$

3. 令$D_{c}$表示训练集$D$中第$c$类样本组成的集合，若有充足的独立同分布 样本，则可容易地估计出类先验概率

   $$P(c) = \frac {|D_c|}{|D|} \tag{16}$$

   - 对离散属性而言，令$D_{c,x_i}$表示$D_c$中在第$i$个属性上取值为$x_i$的样本组成的集合，则条件概率$P(x_i|c)$可估计为

     $$P(x_i|c) = \frac {|D_{c,x_i}|}{|D_c|} \tag{17}$$

   - 对连续属性可考虑概率密度函数，假定$p(x_i|c) \sim \mathcal N(\mu_{c,i},\sigma_{c,i}^2 )$，其中$\mu_{c,i}$和$\sigma_{c,i}^2$分别是第$c$类样本在第$i$个属性上取值的均值和方差，则有

     $$p(x_i|c)  = \frac{1}{\sqrt{2\pi}\sigma_{c,i}^2}exp(-\frac{(x_i-\mu_{c,i})^2}{2\sigma_{c,i}^2})\tag{18}$$

4. 若某个属性值在训练集中没有与某个类同时出现过，则直接基于式(17)进行概率估计$P(x_i|c)$ ，再根据式(15)进行判别时计算出的概率值为零。为了避免其他属性携带的信息被训练集中未出现的属性"抹去"，在估计概率值时通常要进行**平滑**(smoothing)，常用**拉普拉斯修正**(Laplacian correction)，则式(16)和(17)分别修正为

   $$\hat P(C) = \frac{|D_c|+1}{|D|+N}, \tag{19}$$

   $$\hat P(x_i|c) = \frac{|D_{c,x_i}|+1}{|D_c|+N_i},\tag{20}$$

   其中，$N$表示训练集$D$中可能的类别数，$N_i$表示第$i$个属性可能的取值数

5. 拉普拉斯修正避免了因训练集样本不充分而导致概率估值为零的问题，并且在训练集变大时，修正过程所引入的先验(prior)的影响也会逐渐变得可忽略，使得估值渐趋向于实际概率值

   > 拉普拉斯修正实质上假设了属性值与类别均匀分布，这是在朴素贝叶斯学习过程中额外引入的关于数据的先验

6. 在现实任务中朴素贝叶斯分类器有多种使用方式

   - 若任务对预测速度要求较高，则对给定训练集，可将朴素贝叶斯分类器涉及的所有概率估值事先计算好存储起来，这样在进行与测试只需"查表"即可进行判别
   - 若任务数据更替频繁，则可采用**懒惰学习**(lazy learning)方式，先不进行任何训练，待收到预测请求时再根据当前数据集进行估值
   - 若数据不断增加，则可在现有估值基础上，仅对新样本的属性值所涉及的概率估值进行计数修正即可实现增量学习

参考：

- 《机器学习》
- [AI算法工程师手册](<http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/1_linear.html>)
- [南瓜书PumpkinBook](<https://datawhalechina.github.io/pumpkin-book/#/chapter7/chapter7>)