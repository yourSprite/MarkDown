# 逻辑回归

## 一、问题

1. 考虑二分类问题

给定给定数据集$\mathbf{D}$ = {($\mathbf{x}_{1}$,$y_{1}$),($\mathbf{x}_{2}$,$y_{2}$),...,($\mathbf{x}_{m}$,$y_{m}$)}，其中 $\mathbf{x}_{i}$ = {$x_{i1}$;$x_{i2}$;...;$x_{id}$}，$y_{i} \in \left \{ 0, 1\right \}$

- 考虑到$\mathbf{\omega}^{T}\mathbf{x} + b$取值是连续的，因此它不能拟合离散变量

  可以考虑用它来拟合条件概率$p(y = 1\  | \ \mathbf{x})$，因为概率的取值也是连续的

- 但是对于$\mathbf{w}  \neq \mathbf{0}​$ (若等于零向量则没有什么求解的价值)，$z = \mathbf{\omega}^{T}\mathbf{x} + b​$取值是从$-\infty \sim  +\infty​$，不符合概率取值为$0 \sim 1​$，于是，我们需将实值$z​$转换为0/1值

  最理想的是**单位阶跃函数**(unit-step function)

  $$y = \left\{\begin{matrix}
   0,\ \ \ z < 0; & \\
   0.5,\ z = 0;& \\ 
   1,\ \ \ z > 0;& 
  \end{matrix}\right. \tag{1}$$

  即若预测值$z$大于零就判为正例，小于零则判为反例，预测值为临界值零则可任意判别，如图

![pic1](/Users/wangyutian/文档/markdown/pic/逻辑回归/pic1.png)

- 但从图可以看出，单位阶跃函数不满足单调可微，不能直接用作$g(·)$，于是我们希望找到能在一定程度上近似单位阶跃函数的**替代函数**(surrogate function)。**对数几率函数**(logistic function)正式这样一个常用的替代函数

  $$y = \frac{1}{1+e^{-z}} \tag{2}$$

  从图中可以看出，对数几率函数是一种**Sigmoid函数**，它将$z$值转化为一个接近$0$或$1$的$y$值，并且其输出值在$z = 0$附近变化很陡

  > Sigmoid函数即形似S的函数，对率函数是Sigmoid函数最重要的代表

- 采用广义线性模型，得到

  $$y = \frac{1}{1+e^{-(\mathbf{\omega}^{T}\mathbf{x} + b)}} \tag{3}$$

  若将$y = \frac{1}{1+e^{-z}}$视为$x$作为正例的可能性，则$1-y = \frac{e^{-z}}{1+e^{-z}}$是其反例可能性，两者的比值

  $$ \frac{y}{1- y} \tag{4}$$

  称为**几率**(odds)，反映了$x$作为正例的相对可能性

  $$ln( \frac{y}{1- y}) = z =  \mathbf{\omega}^{T}\mathbf{x} + b \tag{5}$$

  对几率取对数则得到**对数几率**(log odds，亦称logit)

  由此可以看出，式(3)实际上是在用线性模型的预测结果去逼近真实标记对数几率，因此，其对应的模型称为**对数几率回归**(logistic regression，亦称logit regression)

2. 虽然对数几率回归名字带有回归，但是它是一种分类的学习方法。其优点

- 直接对分类的可能性进行建模，无需事先假设数据分布，这就避免了因为假设分布不准确带来的问题
- 不仅预测出来类别，还得到了近似概率的预测，这对许多需要利用概率辅助决策的任务有用
- 对数函数是任意阶可导的凸函数，有很好的数学性质，很多数值优化算法都能直接用于求取最优解

## 二、参数估计

1. 给定给定数据集$\mathbf{D}$ = {($\mathbf{x}_{1}$,$y_{1}$),($\mathbf{x}_{2}$,$y_{2}$),...,($\mathbf{x}_{m}$,$y_{m}$)}，其中 $\mathbf{x}_{i}$ = {$x_{i1}$;$x_{i2}$;...;$x_{id}$}，$y_{i} \in \left \{ 0, 1\right \}$。可以用极大似然估计法估计模型参数，从而得出模型。

   为方便讨论，令$\mathbf{\theta} = (\mathbf{\omega};b)$，$\mathbf{x} = (\mathbf{x};1)$，则$\mathbf{\omega}^{T}\mathbf{x} + b$可简写为$\mathbf{\theta}^T\mathbf{x}$，预测函数为

   $$h_{\mathbf{\theta}}(x) = g(\mathbf{\theta}^T\mathbf{x}) = \frac{1}{1+e^{-\mathbf{\theta}^{T}\mathbf{x}}} \tag{6}$$

   其中$\theta_{0} + \theta_{1}x_{1} + ,…, + \ \theta_{m}x_{m} = \sum_{i=1}^m\theta_{i}x_{i} = \mathbf{\theta}^Tx$

2. 对于二分类任务，令$P(y = 1|x;\mathbf{\theta}) = h_{\mathbf{\theta}}(x)$，则$P(y = 0|x;\mathbf{\theta}) = 1- h_{\mathbf{\theta}}(x)$

   整合后

   $$P(y|x;\mathbf{\theta}) = (h_{\mathbf{\theta}}(x))^{y}(1- h_{\mathbf{\theta}}(x))^{1-y} \tag{7}$$

   > 解释：对于二分类任务(0, 1)，整合后y取0只保留$1- h_{\mathbf{\theta}}(x)$，y取1只保留$h_{\mathbf{\theta}}(x)$

3. 由于有$P(y = 1|x;\mathbf{\theta})$ ;$P(y = 0|x;\mathbf{\theta})$，我们可以通过**极大似然法**(maximum likelihood method)来估计$\mathbf{\theta}$

   似然函数

   $$L(\mathbf{\theta}) =\prod_{i = 1}^{m}P(y_{i}|x_{i};\mathbf{\theta}) = \prod_{i = 1}^{m}(h_{\mathbf{\theta}}(x_{i}))^{y_{i}}(1- h_{\mathbf{\theta}}(x_{i}))^{1-y_{i}} \tag{8}$$

   **对数似然**(log likelihood)

   $$l(\mathbf{\theta}) = lnL(\mathbf{\theta}) = \sum_{i=1}^m(y_{i} lnh_{\mathbf{\theta}}(x_{i}) + (1-y_{i})ln(1 - h_{\mathbf{\theta}}(x_{i}))) \tag{9}$$

   式(9)是关于$\mathbf{\theta}$的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如**梯度下降法**(gradient descent method)、**牛顿法**(Newton method)等都可以求得其最优解，于是就得到

   $$\mathbf{\theta^*} = \underset{\mathbf{\theta}}{arg\min}l(\mathbf{\theta}) \tag{10}$$

## 三、求解

对于式(9)，应用梯度上升求最大值，引入$J(\mathbf{\theta}) = -\frac{1}{m}l(\mathbf{\theta})​$转换为梯度下降任务

求解过程

$\frac{\partial J(\mathbf{\theta})}{\theta_{j}} = -\frac{1}{m}\sum_{i=1}^{m}(y_{i}\frac{1}{h_{\theta}(x_{i})}\frac{\partial h_\theta(x_i)}{\partial \theta_j} - (1-y_i)\frac{1}{1-h_{\theta}(x_{i})}\frac{\partial h_\theta(x_i)}{\partial \theta_j})$

$=  -\frac{1}{m}\sum_{i=1}^{m}(y_{i}\frac{1}{g(\mathbf{\theta}^Tx_i)} - (1-y_i)\frac{1}{1-g(\mathbf{\theta}^Tx_i)})\frac{\partial g(\mathbf{\theta}^Tx_i)}{\partial \theta_j}$

$=  -\frac{1}{m}\sum_{i=1}^{m}(y_{i}\frac{1}{g(\mathbf{\theta}^Tx_i)} - (1-y_i)\frac{1}{1-g(\mathbf{\theta}^Tx_i)})g(\mathbf{\theta}^Tx_i)(1-g(\mathbf{\theta}^Tx_i))\frac{\partial \theta^Tx_i}{\partial \theta_j}$

$=  -\frac{1}{m}\sum_{i=1}^{m}(y_{i}(1-g(\mathbf{\theta}^Tx_i)) - (1-y_i)g(\mathbf{\theta}^Tx_i))x_i^j​$

$=  -\frac{1}{m}\sum_{i=1}^{m}(y_i - g(\mathbf{\theta}^Tx_i))x_i^j$

$= \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i) - y_i)x_i^j​$

参数更新

$\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i) - y_i)x_i^j​$


参考

- 《机器学习》
- [AI算法工程师手册](<http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/1_linear.html>)
- [python数据分析与机器学习实战-唐宇迪](<https://study.163.com/course/courseMain.htm?share=1&shareId=1028724809&courseId=1003590004&_trace_c_p_k2_=e0b465adb9914c3dbc8880ec4a23d957>)

- https://study.163.com/course/courseMain.htm?share=1&shareId=1028724809&courseId=1003590004&_trace_c_p_k2_=e0b465adb9914c3dbc8880ec4a23d957>)